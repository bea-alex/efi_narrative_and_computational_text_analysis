{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "touched-register",
   "metadata": {},
   "source": [
    "<img src=\"../images/APISearchBadge.png\" alt=\"Chronicling America Search Example\" width=\"150px\" style=\"border:1px solid black;\" align=\"right\">\n",
    "\n",
    "# Badge 3 - Chronicling America: Conventional Search versus API Search\n",
    "\n",
    "What you'll learn in this Notebook:\n",
    "\n",
    "- What an API is\n",
    "- How it works\n",
    "- How to use it yourself to collect and analyse text\n",
    "\n",
    "## 1. Chronicling America\n",
    "\n",
    "[Chronicling America](https://chroniclingamerica.loc.gov) is a digital repository produced by the Library of Congress which gives access to digitised copies of historical newspapers pages published in the United States from 1777-1963, the U.S. Newspaper Directory of American newspapers published between 1690-present and other resources. Online users can access the data in Chronicling America via a search interface.\n",
    "\n",
    "The screenshot below demonstrates what the website looked like when I accessed it in August 2021.  When you visit the site yourself, it will look slightly different, in that the presented content will refer to the date when you are accessing the page, but structurally it should look fairly similar.\n",
    "\n",
    "<img src=\"../images/CA-homepage.png\" alt=\"Chronicling America Homepage\" width=\"700px\" style=\"border:1px solid black;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-pickup",
   "metadata": {},
   "source": [
    "## 2. Conventional Search\n",
    "\n",
    "Websites whose data can be accessed using conventional search provide users with a search bar, such as the one you will be familiar with from Google. To enable conventional search, Chronicling America needs a **search engine** under the hood of its website. Instead of accessing the whole World Wide Web, however Chronicling America's search engine is limited to the digitised collections it provides access to.\n",
    "\n",
    "The way this works is that all the text in the newspaper pages is indexed, i.e. it is stored in a large **search index**.  A search index can be thought of a bit like a structured encyclopedia. When you look for (or query) a word, it is looked up in the index which can then easily retrieve all the documents (in this case, the newspaper pages) that are relevant to the query.  In the case of Chronicling America, the images of the newspaper pages are also stored along with the digitised text.  So when you look for words, the results are displayed as highlighted text in the relevant documents.\n",
    "\n",
    "Let's start by searching for the phrase \"little alacrity and energy\". You can see the results for this search in the screenshot below. The metadata, given in white underlined text underneath each image of a newspaper page, shows the name, publication date and places of publication for each newspaper.\n",
    "\n",
    "<img src=\"../images/CA-search-example.png\" alt=\"Chronicling America Search Example\" width=\"700px\" style=\"border:1px solid black;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-combining",
   "metadata": {},
   "source": [
    "### üêõ Mini task 2.1\n",
    "\n",
    "Let's look at the newspaper pages which were returned.  Can you explain why they were returned?\n",
    "\n",
    "For example, here is the first result of the 7 pages that were returned:\n",
    "\n",
    "<img src=\"../images/CA-result-1.png\" alt=\"Chronicling America Result 1\" width=\"700px\" style=\"border:1px solid black;\">\n",
    "\n",
    "Here is the last result that was returned. Can you spot anything odd about this example?\n",
    "\n",
    "<img src=\"../images/CA-result-2.png\" alt=\"Chronicling America Result 2\" width=\"700px\" style=\"border:1px solid black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-guest",
   "metadata": {},
   "source": [
    "## API Search\n",
    "\n",
    "### What an API is\n",
    "\n",
    "API is an acronym for Application Programming Interface, which is a complicated way of describing an interface between two applications, or between a human and an application or database.  You can think of an API a bit like a waiter in a restaurant. You know how to talk to the waiter to order food or pay the bill. The waiter acts as the 'interface' between you and the restaurant, bringing you the food you've ordered while shielding you from all the complicated stuff that's going on behind the scenes such as food deliveries, stock management, cooking and cleaning dishes.\n",
    "\n",
    "In the context of searching, an API provides a way to access data systematically using structured queries. A search API allows you to search existing items in a data collection or catalogue, in our case the historical newspaper collections hosted on Chronicling America. In this case, the search API is the interface to the data, and you just need to learn how to communicate with it (i.e. formulate your query using syntax the API can understand) to get the right information back, i.e. to return newspaper pages that are relevant to your search query.\n",
    "\n",
    "A search API allows you to execute a search query using a URL and get back results that match the query. In this sense, it's not that different to conventional search. You just need to learn how the search query URLs are constructed to do the search (instead of typing out your query in the search box and clicking \"Go\", which can be laborious if you have a lot of queries to run). Another useful difference is that with an API, rather than results just being displayed in your browser, you can often specify to download them immediately onto your computer in different formats.\n",
    "\n",
    "Let's see how it works in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-announcement",
   "metadata": {},
   "source": [
    "## Using the Chronicling America API\n",
    "\n",
    "### Simple Text Search\n",
    "\n",
    "All Chronicling API searches start with the following base URL: https://chroniclingamerica.loc.gov/ \n",
    "\n",
    "The base URL is followed by the search, including specific search parameters.\n",
    "\n",
    "E.g. a simple search for articles containing the word \"alacrity\" is made up of the base URL https://chroniclingamerica.loc.gov/ followed by \"search/pages/results/?andtext=alacrity\", so:\n",
    "\n",
    "https://chroniclingamerica.loc.gov/search/pages/results/?andtext=alacrity\n",
    "\n",
    "`/search/pages/results` is an instruction about where to search more specifically (i.e. the content of the pages, rather than other fields such as the newspaper's place of publication), and everything after the question mark specifies the search query and parameters. In this case, we want to search the full text of the newspapers, which is done by using `andtext` set to our search term \"alacrity\" (`andtext=alacrity`).\n",
    "\n",
    "You can also run searches the newspaper titles only or in one or more specific newspapers only (e.g. searching only the pages of the Salt Lake Herald, or those of the Chicago Tribune), but we are interested in searching the text in all of the newspapers in the Chronicling America collection in this tutorial.\n",
    "\n",
    "### Proximate Search\n",
    "\n",
    "If you use Chronicling America's simple browser interface to do a search with multiple words, you are asking it to do what is known as a proximate search. This means that the search engine searches for all the words within each page, but they don't need to appear one after the other, even if you put them in double quotes.  You may have noticed this when inspecting the results of the conventional search earlier, as one page didn't contain the exact search query.\n",
    "\n",
    "When writing a search query for the API, proximate search can be replicated using the `proxtext` search parameter and using plus signs between multiple words.  So, the API search for the equivalent browser search for \"little alacrity and energy\" is:\n",
    "\n",
    "https://chroniclingamerica.loc.gov/search/pages/results/?proxtext=little+alacrity+and+energy\n",
    "\n",
    "If you click on this link, a new tab should open which contains all seven results you looked at earlier.\n",
    "\n",
    "### Phrase Text Search\n",
    "\n",
    "You can also do a search for the exact phrase by using the `phrasetext` parameter, like this:\n",
    "\n",
    "https://chroniclingamerica.loc.gov/search/pages/results/?phrasetext=little+alacrity+and+energy\n",
    "\n",
    "Note that this only returns the six pages containing the phrase \"little alacrity and energy\" but not the page which contains all four individual words but not in the exact phrase specified.  You can achieve the same in the browser search by going to \"Advanced Search\" and running a search with the phrase \"little alacrity and energy\". (If you do not get the number of hits you expect, check the date parameters and make sure you are searching from 1777 to 1963.)\n",
    "\n",
    "But hang on, what's so special about API searches, if I can do the exact same thing in the browser? What are they useful for?\n",
    "\n",
    "### Different Formats and Downloading Results\n",
    "\n",
    "The most useful thing about Search APIs is that they often allow you to download the data in a specified format directly to your computer, so that you can do further analysis with it.\n",
    "\n",
    "If you send a query to the Chronicling America API without specifying a file format, you will receive your results in html format, which is the default, meaning the results are displayed in a new tab in your browser.  So, nothing new there.\n",
    "\n",
    "However, the API also allows you to ask for your results to be delivered in json or atom format. This way you can easily download the files to your computer or bring them into your notebook for further analysis, for instance if you want to use regular expressions to find strings in the text or perhaps perform sentiment analysis on the text.\n",
    "\n",
    "**json** stands for JavaScript Object Notation, which is a standard text-based format for representing structured data based on JavaScript object syntax. It is commonly used for transmitting data in web applications (e.g., sending data from the server to the client, so it can be displayed on a web page, or vice versa).  In our case, it's used to store all the results for a search query.\n",
    "\n",
    "**atom** is the name of an XML-based Web content and metadata syndication format, and an application-level protocol for publishing and editing Web resources belonging to periodically updated websites. So, atom is a type of XML format but we don't use it in this tutorial.\n",
    "\n",
    "We'll be working with the results in json format. For example, if we adapt our previous query so it requests the results in json, it will look like this:\n",
    "\n",
    "https://chroniclingamerica.loc.gov/search/pages/results/?phrasetext=little+alacrity+and+energy&format=json\n",
    "\n",
    "Note that `format=json` is used to request json format and this search parameter is added after an ampersand (&) which separates multiple search parameter key/value pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-argentina",
   "metadata": {},
   "source": [
    "When you click on this link you'll get a new tab which, once it has opened properly, should look like this:\n",
    "<img src=\"../images/CA-results-json.png\" alt=\"Chronicling America Results in json\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-questionnaire",
   "metadata": {},
   "source": [
    "(If your browser loads a page which looks different and has only a few words at top left, try clicking 'Raw Data'.)\n",
    "\n",
    "This is quite hard to read but with an editor which can read json format it's much easier to figure out what's going on.  Guess what? We can load the json into this notebook and look at it.\n",
    "\n",
    "To do this, we first need to import the Python packages `urlopen` and `json`. We then specify the URL, open it using the `urlopen()` function, read the results and load them as json format.  When we call the final `json_results` variable (rather than printing it) then we can see the results with slightly better formatting.\n",
    "\n",
    "When you run the code cell below, you will see that the results contain six items. These correspond to the six newspaper pages that matched our earlier search query, including metadata, such as the title of the newspaper, the place of publication, the publication date, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urlopen and json\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "\n",
    "# store the URL in url variable\n",
    "url = \"https://chroniclingamerica.loc.gov/search/pages/results/?phrasetext=little+alacrity+and+energy&format=json&page=2\"\n",
    "\n",
    "# store the response of URL\n",
    "response = urlopen(url)\n",
    "\n",
    "# read the response and load as json format\n",
    "json_results = json.loads(response.read())\n",
    "  \n",
    "# show what is in the json results\n",
    "json_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-country",
   "metadata": {},
   "source": [
    "You can see that json format uses a key-value pair syntax.  So, the key (which you can think of as a label for the type of information, such as you might find in the header of a column in a table), is on the left, followed by a colon and then, on the right, the value (which you can think of as the information itself, such as you might find partway down a column in a table). Here are some examples of key-value pairs in the data we've just downloaded:\n",
    "\n",
    "```\n",
    "{\n",
    "...\n",
    "'city': ['Washington'],\n",
    "'date': '18400222',\n",
    "'title': 'The native American. [volume]',\n",
    "'publisher': 'J. Elliot Jr.',\n",
    "...\n",
    "}\n",
    "```\n",
    "\n",
    "A value can belong to a range of data types: it can be a number, a string, a Boolean (so true or false), an array (e.g. an ordered list), an object or null (so left empty).\n",
    "\n",
    "A value can be either a number, a string, a Boolean (so true or false), an array (an ordered list of components), an object or null (so left empty).\n",
    "\n",
    "Key-value pairs are separated by commas, and they can also be nested (e.g. if you look at line 5 of the json file, you will see that `items` contains all the items returned by our search query.)\n",
    "\n",
    "You will notice that the result for our query is very long as it contains the OCRed text of each of the six pages (i.e., the text following `'ocr_eng': `).  \n",
    "\n",
    "**OCR** is short for Optical Character Recognition which is used for digitising historical documents, such as the newspapers in Chronicling America.  OCR is used to turn letters in an image into electronic text.\n",
    "\n",
    "Though there have been huge advances in OCR in the recent past, it is not 100% accurate, and if you look at the text more closely you will be able to find some OCR errors.\n",
    "\n",
    "You'll also notice \"\\n\" characters which you should be familiar with from the Regular Expression Notebook.  They signify newline characters.\n",
    "\n",
    "Next, we'll show you how to print the text for each newspaper page a bit more neatly and how to find all matches for \"alacrity\" in them. \n",
    "\n",
    "First, we need to extract the items from our json results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract items from json format\n",
    "items = json_results[\"items\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-motorcycle",
   "metadata": {},
   "source": [
    "The results are stored in a list of items which are still in json format.  To be able to read the information a bit better, let's store it in a table (also known as a data frame), with the information for each item (i.e. the values) being stored in one row, and one column for each type of information (i.e. the keys).\n",
    "\n",
    "To do this, we'll use pandas, as it has a useful function called `json_normalize` which can read information in json format and flatten it into a data frame (which in the code block below is the `df` variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-weapon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.json_normalize(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-embassy",
   "metadata": {},
   "source": [
    "You can now check out the content of the data frame we just created by using the `.head()` function, which you have learnt in an earlier tutorial.  The metadata associated with each newspaper page is now in a table format, and so is much easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-mirror",
   "metadata": {},
   "source": [
    "Note that the data frame even contains the URL pointing to the results for each item (scroll all the way to the right of the dataframe if you cannot see it).  When you follow that URL you'll also be able to get access to other file formats and the images of the newspaper pages themselves.\n",
    "\n",
    "For now, let's look at the OCRed text and see what we can do with it.\n",
    "\n",
    "In the next bit of code, we use a for loop to extract the values of the `ocr_eng` column containing the OCRed text and run a regular expression search over it.  We use the `findall()` function to find all mentions of \"alacrity\".\n",
    "\n",
    "The regular expression used is the following : `'[^\\n]*\\n.*alacrity.*\\n[^\\n]*'`\n",
    "\n",
    "This means \"match any line containing the word 'alacrity' but also the line before and after the line it appears in\" (to show some context).  This looks quite complicated at first, but let's take it apart to understand the different bits of the regular expression and how they work together. Remember that:\n",
    "\n",
    "- `\\n` means newline character\n",
    "- `.*` means zero or more characters\n",
    "- `[^\\n]` means not a newline character\n",
    "- `[^\\n]*` means zero or more characters but not newline\n",
    "\n",
    "So, looking at the first part of the regular expression, `[^\\\\n]*\\\\n*.*alacrity` means \"match zero or more characters but not newline, followed by an optional newline character followed by zero or more characters and the word 'alacrity'\".\n",
    "\n",
    "Then, looking at the second part of the regular expression, `alacrity.*\\\\n[^\\\\n]*` means \"match the word 'alacrity', followed by zero or more characters, followed by newline and zero or more characters but not newline\".\n",
    "\n",
    "So, together, the RegEx means \"match the line containing the word 'alacrity' as well as the line before and after, if there is one\".  This might seem difficult but when you use regular expressions frequently, you'll become used to reading and constructing them.\n",
    "\n",
    "The `re.IGNORECASE` flag at the end makes the search case-insensitive.\n",
    "\n",
    "Finally, we'll store the matches from our RegEx search in a list (`all_results`) and then use another for loop to print them more neatly, displaying the number of the item, followed by the match in context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-transparency",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "all_results=[]\n",
    "for (index, row) in df.iterrows():\n",
    "    item = row.loc['ocr_eng']\n",
    "    results=re.findall('[^\\n]*\\\\n*.*alacrity.*\\n[^\\n]*', item, re.IGNORECASE)\n",
    "    all_results.append(results)\n",
    "\n",
    "counter=0\n",
    "for results in all_results:\n",
    "    counter=counter+1\n",
    "    print(\"Item \" + str(counter) + \":\")\n",
    "    for result in results:\n",
    "        print(result + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-arthur",
   "metadata": {},
   "source": [
    "Look how far you've come. You have downloaded the results for a search query using an API, have converted the data into a data frame, have extracted the textual information, have matched some text using a Regular Expression search and have displayed the results in your notebook. Well done!\n",
    "\n",
    "Now the power of using API search should be clearer.  It's very useful for searching and accessing data collections hosted online and especially so for analysing large numbers of results.  Imagine our search retrieved not seven but hundreds or thousands of results.  Using API search combined with Python RegEx search allows you to extract and analyse textual data much faster than having to navigate through it all manually in a browser.\n",
    "\n",
    "\n",
    "### More advanced API-driven data collection\n",
    "\n",
    "Up to now, we used quite a specific search query which only returns a few results.  In the following code cell, you'll see how we can collect data for an API search query that returns more results.\n",
    "\n",
    "After the imports, you'll see a function called `safe_api_call` which you can use to ensure that the request to the API is successful and returns some results.  In some cases, an API might be down or a user might be blocked because they have sent too many queries to an API.  So this function checks that the API serves a response as it should.  The code only continues to run if the API returns a result. \n",
    "\n",
    "If so, the code then determines how many results the query returns, so this tells us the number of documents (items) containing the search query.  In our previous example, we only worked with 7 results.  In the following, you'll learn how to collect and analyse data if the query returns a lot more results.\n",
    "\n",
    "For example, for the query (`black cat`) below, we get over 53,500 results with 20 results per page, so 2,678 pages of results.  In a real browser, you would most likely only look at the first few pages of results to find  information that's relevant to you and, in some cases, re-fine your query to get fewer, more specific results.\n",
    "\n",
    "Using an API, you could collect and analyse all of the data. However, this would take quite a long time to do.  So in this case, we cap the number of results at 100.  This is mainly to show how this kind of data collection work in principle and also to not overload the API.  In some cases, APIs have constraints which allow you search them only up to X times per minute, hour or day and if you go over this limit, then your computer will be blocked or blacklisted.  We want to avoid that from happening when doing these exercises.\n",
    "\n",
    "While the following code cell contains quite a substantive bit of code, it basically breaks down into some safety checking code to make sure the API works as it should, a for loop which goes through all of the pages and collects the results, and some code which stores the results into one overall data frame (`all_df`).  Each bit of code contains some print statements to provide an idea of what's currently happening as the code is run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c81df31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the imports needed to make this bit of code run\n",
    "import requests\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "# this safe_api_call function is an extra level of checking that the API returns results as it should be\n",
    "# sometimes it might be down or overloaded, in which case you have to wait and come back to do this another time\n",
    "# this function checks a URL three times to see if it loads properly.  If it does then the results are returned.\n",
    "# If it doesn't then None is returned.\n",
    "def safe_api_call(url, max_retries=3):\n",
    "    \"\"\"Make API call with error handling and retries\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            print(f\"Status code: {response.status_code}\")\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                # Check if response is actually JSON\n",
    "                content_type = response.headers.get('content-type', '')\n",
    "                if 'application/json' in content_type:\n",
    "                    return response.json()\n",
    "                else:\n",
    "                    print(f\"Expected JSON but got: {content_type}\")\n",
    "                    print(f\"Response preview: {response.text[:200]}\")\n",
    "                    return None\n",
    "            else:\n",
    "                print(f\"HTTP Error {response.status_code}: {response.text[:200]}\")\n",
    "                return None\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed (attempt {attempt + 1}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                return None\n",
    "        except ValueError as e:\n",
    "            print(f\"JSON parsing failed (attempt {attempt + 1}): {e}\")\n",
    "            print(f\"Response content: {response.text[:200]}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "# Main code with error handling\n",
    "# First we specify a query and add it to the base url.\n",
    "query = \"black+cat\"\n",
    "url = \"https://chroniclingamerica.loc.gov/search/pages/results/?format=json&phrasetext=\" + query\n",
    "\n",
    "# Trying to get a response from the API using the URL\n",
    "print(f\"Trying URL: {url}\")\n",
    "json_results = safe_api_call(url)\n",
    "\n",
    "# If the result is None then the code is exited\n",
    "if json_results is None:\n",
    "    print(\"API call failed. Cannot proceed.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# The rest of the code only continues if API call succeeeds\n",
    "# Here we get the overall number of results and pages for the search query\n",
    "overall_count = json_results[\"totalItems\"]\n",
    "\n",
    "# By default Chronicling America returns 20 results per page, so we can work out the number of pages\n",
    "# The math.ceil() function rounds up the results to the next integer\n",
    "pages = math.ceil(overall_count / 20)\n",
    "print(\"There are \" + str(overall_count) + \" hits (presented on \" + str(pages) + \" pages) for the search term: \" + query)\n",
    "\n",
    "# Here we create an empty data frame in which we will store all there results collected\n",
    "all_df = pd.DataFrame()\n",
    "\n",
    "# If there are more than 5 pages (more than 100) results then we'll cap the data collection at 100 results.\n",
    "# Otherwise we'll collect all results\n",
    "if pages > 5:\n",
    "    print(\"To avoid overloading the API and getting blocked, we will only collect data for the first 100 documents.\")\n",
    "    pages = 5\n",
    "else:\n",
    "    print(\"Collecting all results\")\n",
    "\n",
    "# This bit of code loops through the number of pages (5 or lower)\n",
    "# and downloads the results for each page\n",
    "for page in range(1, pages + 1):\n",
    "    print(\"Downloading results for page: \" + str(page))\n",
    "\n",
    "    # We add a short 2 second break inbetween each API call to not overload it\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # The page_url is the previous url containing the query with two additional paramaters added at the end:\n",
    "    # the number of results per page (rows) and the number of the page (page)\n",
    "\n",
    "    page_url = f\"{url}&rows=20&page={page}\"\n",
    "    \n",
    "    # As we are in the loop, the next line of code collects the results for each page\n",
    "    json_results = safe_api_call(page_url)\n",
    "    \n",
    "    # These two if statements check that each page loads and contains results, if not the the loop continues\n",
    "    # with the next page\n",
    "    if json_results is None:\n",
    "        print(f\"Failed to get page {page}, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    if \"items\" not in json_results:\n",
    "        print(f\"No items found on page {page}\")\n",
    "        continue\n",
    "        \n",
    "    # Here we extract the results for each of the documents (items)\n",
    "    # and add them to the data frame which we created earlier.\n",
    "    items = json_results[\"items\"]\n",
    "    df = pd.json_normalize(items)\n",
    "    all_df = pd.concat([all_df, df], ignore_index=True)\n",
    "\n",
    "    print(f\"Total records so far: {len(all_df)}\")\n",
    "\n",
    "# This prints the length of the dataframe, i.e. how many documents that match the query it contains.\n",
    "# As the query \"cat\" returns way too many results we only collect the first 100.\n",
    "print(f\"Final dataset contains {len(all_df)} documents.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d28380",
   "metadata": {},
   "source": [
    "Take a look at the data frame to see what documents were collected for the search query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc80b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71857b8f",
   "metadata": {},
   "source": [
    "Following on from here, you can run the code we used before to print out the text containing the search query.  The output will contain results for 100 documents (unless the query returned less results).  Note that in some cases, a document contains more than one match.\n",
    "\n",
    "In the next code cell, the code prints the results is a bit more sophisticated that we did before, in that it prints out the context of the query (previous and next two lines of text, if there are any).  It also prints a delimiter between documents, so you can more easily see what results were returned for each document.\n",
    "\n",
    "Now, take a look at the output to see in what contexts the phrase \"black cat\" is used in the data you obtained from Chronicling America."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d7d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "all_results = []\n",
    "\n",
    "# This for loop goes through the data from and grabs the text (orc_eng) for each matched document\n",
    "for (index, row) in all_df.iterrows():\n",
    "    item = row.loc['ocr_eng']\n",
    "    \n",
    "    # This splits the text into lines\n",
    "    lines = item.split('\\n')\n",
    "    \n",
    "    # A list which will store the lines that contain the query and their context\n",
    "    matches_with_context = []\n",
    "    \n",
    "    # This for loop goes through the lines in each document\n",
    "    for i, line in enumerate(lines):\n",
    "        # And searches for the query (case-insensitively)\n",
    "        if re.findall(\"black cat\", line, re.IGNORECASE):\n",
    "\n",
    "            # Gets 2 lines before and 2 lines after the mached line\n",
    "            start_idx = max(0, i - 2)\n",
    "            end_idx = min(len(lines), i + 3)  # +3 because range is exclusive at end\n",
    "            \n",
    "            # Extract the context lines with line numbers\n",
    "            context_lines_with_numbers = []\n",
    "            for j in range(start_idx, end_idx):\n",
    "                line_num = j + 1  # Line numbers start at 1\n",
    "                line_content = lines[j]\n",
    "            \n",
    "                # Format with line number\n",
    "                formatted_line = f\"{line_num:4d}: {line_content}\"\n",
    "                context_lines_with_numbers.append(formatted_line)\n",
    "            \n",
    "            matches_with_context.append('\\n'.join(context_lines_with_numbers))\n",
    "    \n",
    "    all_results.append(matches_with_context)\n",
    "    \n",
    "counter = 0\n",
    "for results in all_results:\n",
    "    counter = counter + 1\n",
    "    print(\"Document \" + str(counter) + \":\\n\")\n",
    "    for result in results:\n",
    "        print(result+\"\\n\")  \n",
    "    print(\"\\n\"+\"=\"*50 + \"\\n\") # Added separator for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-exchange",
   "metadata": {},
   "source": [
    "### ü¶ã Final Task \n",
    "\n",
    "We would now like you to explore the Chronicling America newspaper data using your own searches. Try out some conventional searches of interest that return different numbers of results and then adapt the code above to do the same searches using Search API, download the data in json and find text snippets within the OCRed text using a regular expressions.\n",
    "\n",
    "Note when you adapt code, it's best to take a copy of the existing code and change the copy, so that you still have the original code working as intended.  So create some code cells below using the plus button (top left), copy the code and adapt it to your searches.  Avoid using the scissor button.  It deletes cells, though you can get them back by clicking on \"Edit\" -> \"Undo Delete Cells\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your solution here and create some further code cells for additional code\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

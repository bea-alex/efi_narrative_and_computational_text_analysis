{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "touched-register",
   "metadata": {},
   "source": [
    "# Chronicling America: Conventional Search versus API Search\n",
    "\n",
    "What you'll learn in this Notebook:\n",
    "\n",
    "- What an API is\n",
    "- How it works\n",
    "- How to run it yourself\n",
    "\n",
    "## 1. Chronicling America\n",
    "\n",
    "[Chronicling America](https://chroniclingamerica.loc.gov) is a digital repository produced by the Library of Congress which gives access to digitised copies of historical newspapers pages published in the United States from 1777-1963, the U.S. Newspaper Directory of American newspapers published between 1690-present and other resources. Online users can access the data in Chronicling America via a search interface.\n",
    "\n",
    "The screenshot below demonstrates what the website looked like when I accessed it in August 2021.  When you visit the site yourself, it will look slightly different, in that the presented content will refer to the date when you are accessing the page, but structurally it should look fairly similar.\n",
# BEA IS IT POSSIBLE TO ADD A BORDER TO THE SCREENSHOT IMAGE?
    "\n",
    "<img src=\"../images/CA-homepage.png\" alt=\"Chronicling America Homepage\" width=\"700px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-pickup",
   "metadata": {},
   "source": [
    "## 2. Conventional Search\n",
    "\n",
    "Websites whose data can be accessed using conventional search provide users with a search bar, such as the one you will be familiar with from Google. To enable conventional search, Chronicling America needs something like a **search engine** under the hood of its website. Instead of accessing the whole World Wide Web, however Chronicling America's search engine is limited to the digitised collections it provides access to.\n",
    "\n",
    "The way this works is that all the text in the newspaper pages is indexed, i.e. it is stored in a large **search index**.  A search index can be thought of a bit like a structured encyclopedia. When you look for (or query) a word, it is looked up in the index which can then easily retrieve all the documents (in this case, the newspaper pages) that are relevant to the query.  In the case of Chronicling America, the images of the newspaper pages are also stored along with the digitisedtext.  So when you look for words, the results are displayed as highlighted text in the relevant documents.\n",
    "\n",
    "Let's start by searching for the phrase \"little alacrity and energy\". You can see the results for this search in the screenshot below. The metadata, given in white underlined text underneath each image of a newspaper page, shows the name, publication date and places of publication for each newspaper.\n",
    "\n",
    "<img src=\"../images/CA-search-example.png\" alt=\"Chronicling America Search Example\" width=\"700px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-combining",
   "metadata": {},
   "source": [
    "### üêõ Mini task 2.1\n",
    "\n",
    "Let's look at the newspaper pages which were returned.  Can you explain why they were returned?\n",
    "\n",
    "For example, here is the first result of the 7 pages that were returned:\n",
    "\n",
    "<img src=\"../images/CA-result-1.png\" alt=\"Chronicling America Result 1\" width=\"700px\">\n",
    "\n",
    "Here is the last result that was returned. Can you spot anything odd about this example?\n",
    "\n",
    "<img src=\"../images/CA-result-2.png\" alt=\"Chronicling America Result 2\" width=\"700px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-guest",
   "metadata": {},
   "source": [
    "## API Search\n",
    "\n",
    "### What an API is\n",
    "\n",
    "API is an acronym for Application Programming Interface, which is a complicated way of describing an interface between two applications, or between a human and an application or database.  You can think of an API a bit like a waiter in a restaurant. You know how to talk to the waiter to order food or pay the bill. The waiter acts at the 'interface' between you and the restaurant, bringing you the food you've ordered while shielding you from all the complicated stuff that's going on behind the scenes such as food deliveries, stock management, cooking and cleaning dishes.\n", 
#BEA I LOVE THIS ANALOGY!
    "\n",
    "In the context of searching, an API provides a way to access data systematically using structured queries. A search API allows you to search existing items in a data collection or catalogue, in our case the historical newspaper collections hosted on Chronicling America. In this case, the search API is the interface to the data, and you just need to learn how to communicate with it (ie. formulate your query using syntax the API can understand) to get the right information back, i.e. to return newspaper pages that are relevant to your search query.\n",
    "\n",
    "A search API allows you to execute a search query using a URL and get back results that match the query. In this sense, it's not that different to conventional search. You just need to learn how the search query URLs are constructed to do the search (instead of typing out your query in the search box and clicking \"Go\", which can be laborious if you have a lot of queries to run). Another useful difference is that with an API, rather than results just being displayed in your browser, you can often specify to download them immediately onto your computer in different formats.\n",
    "\n",
    "Let's see how it works in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-announcement",
   "metadata": {},
   "source": [
    "## Using the Chronicling America API\n",
    "\n",
    "### Simple Text Search\n",
    "\n",
    "All Chronicling API searches start with the following base URL: https://chroniclingamerica.loc.gov/ \n",
    "\n",
    "The base URL is followed by the search, including specific search parameters.\n",
    "\n",
    "E.g. a simple search for articles containing the word \"alacrity\" is made up of the base URL https://chroniclingamerica.loc.gov/ followed by \"search/titles/results/?andtext=alacrity\", so:\n",
    "\n",
    "https://chroniclingamerica.loc.gov/search/titles/results/?andtext=alacrity\n",
    #BEA, I MIGHT BE WRONG ABOUT THIS BUT SHOULD 'titles' BE REPLACED BY 'pages', AS THIS IS WHAT YOU'VE GOT IN THE EXAMPLES BELOW? IE SO THE QUERY IS https://chroniclingamerica.loc.gov/search/pages/results/?andtext=alacrity ?
    "\n",
    "`/search/pages/results` is an instruction about where to search more specifically (ie. the content of the pages, rather than other fields such as  the newspaper's place of publication), and everything after the question mark specifies the search query and parameters. In this case, we want to search the full text of the newspapers, which is done by using `andtext` set to our search term \"alacrity\" (`andtext=alacrity`).\n",
    "\n",
    "You can also run searches in one or more specific newspapers only (eg. searching only the pages of the Salt Lake Herald, or those of the Chicago Tribune), but we are interested in searching the text in all of the newspapers in the Chronicling America collection in this tutorial.\n",
    "\n",
    "### Proximate Search\n",
    "\n",
    "If you use Chronicling America's simple browser interface to do a search with multiple words, you are asking it to do what is known as a proximate seaarch. This means that the search engine searches for all the words within each page, but they don't need to appear one after the other, even if you put them in double quotes.  You may have noticed this when inspecting the results of the conventional search earlier, as one page didn't contain the exact search query.\n",
    "\n",
    "When writing a search query for the API, proximate search can be replicated using the `proxtext` search parameter and using plus signs between multiple words.  So, the API search for the equivalent browser search for \"little alacrity and energy\" is:\n",
    "\n",
    "https://chroniclingamerica.loc.gov/search/pages/results/?&proxtext=little+alacrity+and+energy\n",
    #BEA, IMPORTANT CHANGE TO URL IN LINE IMMEDIATELY ABOVE - THE LINK WOULDN'T WORK FOR ME UNTIL I INSERTED A ? BEFORE &protext
    "\n",
    "If you click on this link, a new tab should open which contains all seven results you looked at earlier.\n",
    "\n",
    "### Phrase Text Search\n",
    "\n",
    "You can also do a search for the exact phrase by using the `phrasetext` parameter, like this:\n",
    "\n",
    "https://chroniclingamerica.loc.gov/search/pages/results/?phrasetext=little+alacrity+and+energy\n",
    #THIS URL DOES WORK, BUT FOR CONSISTENCY SHOULD IT ALSO BE &?phrasetext (RATHER THAN ?phrasetext)?
    "\n",
    "Note that this only returns the six pages containing the phrase \"little alacrity and energy\" but not the page which contains all four individual words but not in the exact phrase specified.  You can achieve the same in the browser search by going to \"Advanced Search\" and running a search with the phrase \"little alacrity and energy\". (If you do not get the number of hits you expect, check the date parameters and make sure you are searching from 1777 to 1963.)\n",
    "\n",
    "But hang on, what's so special about API searches, if I can do the exact same thing in the browser? What are they useful for?\n",
    "\n",
    "### Different Formats and Downloading Results\n",
    "\n",
    "The most useful thing about Search APIs is that they often allow you to download the data in a specified format directly to your computer, so that you can do further analysis with it.\n",
    "\n",
    "If you send a query to the Chronicling America API without specifying a file format, you will receive your results in html format, which is the default, meaning the results are displayed in a new tab in your browser.  So, nothing new there.\n",
    "\n",
    "However, the API also allows you to ask for your results to be delivered in json or atom format. This way you can easily download the files to your computer or bring them into your notebook for further analysis, for instance if you want to use regular expressions to find strings in the text or perhaps perform sentiment analysis on the text.\n",
   # NICE LINK!
    "\n",
    "**json** stands for JavaScript Object Notation, which is a standard text-based format for representing structured data based on JavaScript object syntax. It is commonly used for transmitting data in web applications (e.g., sending data from the server to the client, so it can be displayed on a web page, or vice versa).  In our case, it's used to store all the results for a search query.\n",
    "\n",
    "**atom** is the name of an XML-based Web content and metadata syndication format, and an application-level protocol for publishing and editing Web resources belonging to periodically updated websites. So, atom is a type of XML format but we don't use it in this tutorial.\n",
    "\n",
    "We'll be working with the results in json format. For example, if we adapt our previous query so it requests the results in json, it will look like this:\n",
    "\n",
    "https://chroniclingamerica.loc.gov/search/pages/results/?phrasetext=little+alacrity+and+energy&format=json"
   ]
 # BEA, AGAIN, SHOULD THIS HAVE A & BEOFRE THE ? FOR CONSISTENCY? NOT SURE BUT JUST MENTIONING IT IN CASE
  },
  {
   "cell_type": "markdown",
   "id": "exact-argentina",
   "metadata": {},
   "source": [
    "When you click on this link you'll get a new tab which, once it has opened properly, should look like this:\n",
    "<img src=\"../images/CA-results-json.png\" alt=\"Chronicling America Results in json\">"
   ]
   # BEA MY BROWSER DOES SOMETHING DIFFERENT: AT FIRST IT LOADS SOMETHING THAT RESEMBLES YOUR SCREENSHOT, THEN IT CHANGES TO A PAGE WHICH HAS 'JSON / Raw data / Headers' UP THE TOP. SO I'VE ADDED AN EXTRA LINE BELOW IN CASE THIS HAPPENS TO ANYONE IN THE CLASS AS WELL
  },
  {
   "cell_type": "markdown",
   "id": "interested-questionnaire",
   "metadata": {},
   "source": [
    "(If your browser loads a page which looks different and has only a few words at top left, trying clicking 'Raw Data'.)
    "This is quite hard to read, but with an editor which can read json format it's much easier to figure out what's going on. Guess what? We can load the json into this notebook and look at it.\n",
    "\n",
    "To do this, we first need to import the Python packages `urlopen` and `json`. We then specify the URL, open it using the `urlopen()` function, read the results and load them as json format.  When we call the final `json_results` variable (rather than printing it) then we can see the results with slightly better formatting.\n",
    "\n",
    "When you run the code cell below, you will see that the results contain six items. These correspond to the six newspaper pages that matched our earlier search query, including metadata, such as the title of the newspaper, the place of publication, the publication date, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urlopen and json\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "\n",
    "# store the URL in url variable\n",
    "url = \"https://chroniclingamerica.loc.gov/search/pages/results/?phrasetext=little+alacrity+and+energy&format=json\"\n",
    "  \n",
    "# store the response of URL\n",
    "response = urlopen(url)\n",
    "\n",
    "# read the response and load as json format\n",
    "json_results = json.loads(response.read())\n",
    "  \n",
    "# show what is in the json results\n",
    "json_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-country",
   "metadata": {},
   "source": [
    "You can see that json format uses a key-value pair syntax.  So, the key (which you can think of as a label for the type of information, such as you might find in the heading of a column in a table), is on the left, followed by a colon and then, on the right, the value (which you can think of as the information itself, such as you might find partway down a column in a table). Here are some examples of key-value pairs in the data we've just downloaded:\n",
    "\n",
    "```\n",
    "{\n",
    "...\n",
    "'city': ['Washington'],\n",
    "'date': '18400222',\n",
    "'title': 'The native American. [volume]',\n",
    "'publisher': 'J. Elliot Jr.',\n",
    "...\n",
    "}\n",
    "```\n",
    "\n",
    "A value can belong to a range of data types: it can be a number, a string, a Boolean (so true or false), an array (an ordered list of components [SORRY BEA I DON'T THINK THEY'LL KNOW WHAT COMPONENTS ARE. WOULD 'A KIND OF ORDERED LIST' WORK? OR SOMETHING ELSE? I'M NOT ENTIRELY SURE WHAT AN ARRAY IS MYSELF, I MUST ADMIT ...]), an object or null (so left empty). \n",
    "\n",
    "Key-value pairs are separated by commas, and they can also be nested (eg. if you look at line 5 of the json file, you will see that `items` contains all the items returned by our search query.)\n",
    "\n",
    "You will notice that the result for our query is very long as it contains the OCRed text of each of the six pages (i.e., the text following `'ocr_eng': `).  \n",
    "\n",
    "**OCR** is short for Optical Character Recognition which is used for digitising historical documents, such as the newspapers in Chronicling America.  OCR is used to turn letters in an image into electronic text.\n",
    "\n",
    "Though there have been huge advances in OCR in the recent past, it is not 100% accurate, and if you look at the text more closely you will be able to find some OCR errors.\n",
    "\n",
    "You'll also notice \"\\n\" characters which you should be familiar with from the Regular Expression notebook. They signify newline characters.\n",
    "\n",
    "Next, we'll show you how to print the text for each newspaper page a bit more neatly and how to find all matches for \"alacrity\" in them. \n",
    "\n",
    "First, we need to extract the items from our json results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract items from json format\n",
    "items = json_results[\"items\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-motorcycle",
   "metadata": {},
   "source": [
    "The results are stored in a list of items which are still in json format.  To be able to read the information a bit better, let's store it in a table (also known as a data frame), with the information for each item (ie. the values) being stored in one row, and one column for each type of information (ie. the keys).\n",
    "\n",
    "To do this, we'll use pandas, as it has a useful function called `json_normalize` which can read information in json format and flatten it into a data frame (which in the code block below is the `df` variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-weapon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.json_normalize(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-embassy",
   "metadata": {},
   "source": [
    "You can now check out the content of the data frame we just created by using the `.head()` function, which you have learnt in an earlier tutorial.  The metadata associated with each newspaper page is now in a table format, and so is much easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-mirror",
   "metadata": {},
   "source": [
    "Note that the data frame even contains the URL pointing to the results for each item (scroll all the way to the right of the dataframe if you cannot see it).  When you follow that URL you'll also be able to get access to other file formats and the images of the newspaper pages themselves.\n",
    "\n",
    "For now, let's look at the OCRed text and see what we can do with it.\n",
    "\n",
    "In the next bit of code, we use a for loop to extract the values of the `ocr_eng` column containing the OCRed text and run a regular expression search over it.  We use the `findall()` function to find all mentions of \"alacrity\".  \n",
    "\n",
    "The regular expression used is the following : `'[^\\n]*\\n.*alacrity.*\\n[^\\n]*'`\n",
    "\n",
    "This means \"match any line containing the word \"alacrity\" but also the line before and after the line it appears in\" (to show some context).  This looks quite complicated at first, but let's take it apart to understand the different bits of the regular expression and how they work together. Remember that:\n",
    "\n",
    "- `\\n` means newline character\n",
    "- `.*` means zero or more characters\n",
    "- `[^\\n]` means not a newline character\n",
    "- `[^\\n]*` means zero or more characters but not newline\n",
    "\n",
    "So, looking at the first part of the regular expression, `[^\\n]*\\n.*alacrity` means \"match zero or more characters but not newline, followed by a newline character followed by zero or more characters and the word \"alacrity\"\".\n",
    "\n",
    "Then, looking at the second part of the regular expression, `alacrity.*\\n[^\\n]*` means \"match the word \"alacrity\", followed by zero or more characters, followed by newline and zero or more characters but not newline\".\n",
    "\n",
    "So, together, the RegEx means \"match the line containing the word \"alacrity\" as well as the line before and after that line, if they exist\".  This might seem difficult but when you use regular expressions frequently, you'll become used to reading and constructing them.\n",
    "\n",
    "Finally, we'll store the matches from our RegEx search in a list (`all_results`) and then use another for loop to print them more neatly, displaying the number of the item, followed by the match in context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-transparency",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "all_results=[]\n",
    "for (index, row) in df.iterrows():\n",
    "    item = row.loc['ocr_eng']\n",
    "    results=re.findall('[^\\n]*\\n.*alacrity.*\\n[^\\n]*', item)\n",
    "    all_results.append(results)\n",
    "\n",
    "counter=0\n",
    "for results in all_results:\n",
    "    counter=counter+1\n",
    "    print(\"Item \" + str(counter) + \":\")\n",
    "    for match in results:\n",
    "        print(match + \"\\n\")\n",
    "    \n",
    "    #^.*\\bdog and gorilla\\b.*\\r?\\n(?:.*\\r?\\n){2}((?:.*\\r?\\n){3})"
   ]
   #BEA I DON'T UNDERSTAND WHY THE 'bdog and gorilla' COMMENT IS IN THERE AT THE END?
  },
  {
   "cell_type": "markdown",
   "id": "imported-arthur",
   "metadata": {},
   "source": [
    "Look how far you've come. You have downloaded the results for a search query using an API, converted the data into a data frame, extracted the textual information from that data, matched some text using a Regular Expression search and then displayed the results in your notebook. Well done!\n",
    "\n",
    "Now the power of using API search should be clearer.  It's very useful for searching and accessing data collections hosted online and especially so for analysing large numbers of results.  Imagine our search retrieved not seven but hundreds or thousands of results.  Using API search combined with Python RegEx search allows you to extract and analyse textual data much faster than having to navigate through it all manually in a browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-athletics",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
# FANTASTIC NOTEBOOK! THANK YOU!!

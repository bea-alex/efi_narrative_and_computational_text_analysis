{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "seeing-hours",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Badge 1: Tokenising Text and Text in Context <img src=\"../images/badge01.png\" width=140 height=140 style=\"float: right;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-torture",
   "metadata": {},
   "source": [
    "In this notebook, you will learn how to:\n",
    "\n",
    "1. [**Load a Text File** in Your Notebook](#1)\n",
    "2. [**Tokenise** Strings: Splitting Them into Tokens (Words, etc.)](#2)\n",
    "3. [Create **Concordance** Lists (tokens in context)](#3)\n",
    "4. [Search Through Text Using **Regular Expressions** (wildcards syntax)](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-respect",
   "metadata": {},
   "source": [
    "### Recap of syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-facing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list, get its first item and get its length.\n",
    "my_letters = [\"A\",\"B\",\"C\",\"D\",\"E\"] \n",
    "print(my_letters[0])\n",
    "print(len(my_letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-examination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice of items: from the first item up to (not including) the second-to-last item\n",
    "print(my_letters[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-worth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Comprehend' a list as its lowercase values:\n",
    "my_letters_lowercased = [letter.lower() for letter in my_letters]\n",
    "print(my_letters_lowercased) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "serial-finder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also: run this cell now. It's the usual imports of text mining libraries.\n",
    "import nltk\n",
    "import numpy\n",
    "import string\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-perspective",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# 1. Loading a Text File in Your Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-crest",
   "metadata": {},
   "source": [
    "#### Questions & Objectives\n",
    "\n",
    "- How can I load a text file from my hard drive?\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "- To open and read a file on your computer, we use `open()`, `read()` and `close()`\n",
    "- Once the file is opened, you can store its contents in a variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-draft",
   "metadata": {},
   "source": [
    "Broadly speaking there are two contexts in which we load a text file for analysis:\n",
    "\n",
    "- Local file:  you have your file on your [virtualized] computer or hard drive because you created or downloaded it earlier\n",
    "- Remote file: you access the file directly from some website, 'on the fly', processing it with your code but never really saving it as your own (e.g., for copyright or convenience reasons)\n",
    "\n",
    "We are only covering the first case in this badge as the text transcript file as we provide that to you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-planning",
   "metadata": {},
   "source": [
    "### Loading a local file:\n",
    "\n",
    "First let's load a file from your 'hard drive' - because we are working inside of Noteable, it acts as your hard drive. There's a file you downloaded called `Babysitting-transcript.txt` and it is in the data folder, so we reference it with the path of where to find it using the notebook folder as a reference `../data/Babysitting-transcript.txt` (the `../` means 'up one folder from this one')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "duplicate-implementation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of characters: 47819\n",
      "**Babysitting\n",
      "*Prologue\n",
      "\n",
      "@Mother\tAll right, guys. We're here. Don't forget your stuff, OK? And Dylan, grab your snow pants.\n",
      "\n",
      "@Male Child\tOK.\n",
      "\n",
      "@Ira Glass\tHere's a ritual that happens in millions of American families every day, parents dropping off kids at the babysitter's.\n",
      "\n",
      "@Cristiana\tGood morning.\n",
      "\n",
      "@Mother\tGood morning.\n",
      "\n",
      "@Cristiana\tHi, sweetie. I haven't seen you guys in such a long time.\n",
      "\n",
      "@Ira Glass\tSarah, age 9, and Dylan, who's 6, are being left at a friend's house where there are two other kids, Elliott and Emma, and their regular babysitter, Cristiana, who meets them at the door, who hasn't seen them since before Christmas.These kids have known Cristiana longer than they've known almost anyone. Four years she's been their sitter, an eternity. Cristiana takes care of them after school every day. Cristiana knows everything about them. And their such old pros at being left with the sitter, they don't think twice about it. Mom leaves, no te\n",
      "------------\n",
      "\n",
      "@Ira Glass\tToday's program of our show was produced by Alex Bloomberg and myself, with [INAUDIBLE], Jonathan Goldstein, Starlee Klein, Julie Snyder, and Aaron Yankee. Our technical director is Matt Tierney. Production up for today's show by BA Parker.Quick program note, a couple weeks ago, we had an excerpt from Jon Ronson's new podcast, The Butterfly Effect, on our program. He has been one of my favorite interviewers anywhere, a wonderful writer. And his podcast has just been released. If you want to hear it, it's an audible.com/butterfly. Our website, ThisAmericanLife.org.This American Life is delivered to public radio stations by PRX, the Public Radio Exchange. Thanks as always to our program's co-founder, Mr. Torey Malatia. He still asks me after every single show, you tell them how much it cost? You tell them how long I had to work to buy that? You did not.I'm Ira Glass. Back next week with more stories of This American Life.\n",
      "\n",
      "[MUSIC]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_name = \"../data/Babysitting-transcript.txt\"\n",
    "my_file = open(file_name) # open the file\n",
    "transcript = my_file.read()   # read contents of the file and put them in a variable\n",
    "my_file.close()           # close the file\n",
    "\n",
    "# After that you have access to the file as text in the transcript variable you created.\n",
    "print(\"number of characters:\", len(transcript)) \n",
    "print(transcript[:955])       # first 955 characters\n",
    "print(\"------------\")\n",
    "print(transcript[-955:])      # last 955 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-announcement",
   "metadata": {},
   "source": [
    "### So now we have a long string ... what's next?\n",
    "\n",
    "As we can see it is not particularly useful to operate on **characters** as the main measure of length and to access parts of text. It would be more meaningful to ask for the first 10 words or last 10 words. Indeed, we might want to consider punctuation and symbols too.\n",
    "\n",
    "This is where tokens come in:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-wallpaper",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "# 2. Tokenising Strings: Splitting Text into Tokens (Words, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-mozambique",
   "metadata": {},
   "source": [
    "#### Questions & Objectives\n",
    "\n",
    "- What is tokenisation?\n",
    "- How can a string of raw text be tokenised?\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "- Tokenisation means to split a string into separate words and punctuation marks, to be able to, for example, count them.\n",
    "- Text can be tokenised using a tokeniser, e.g., the `punkt` tokeniser in NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-layer",
   "metadata": {},
   "source": [
    "In order to process text we need to break it down into tokens. As we explained at the start, a token is a letter, word, number, or punctuation mark which is contained in a string.\n",
    "\n",
    "To tokenise we first need to import the `word_tokenize` method from the `tokenize` package of NLTK, which allows us to tokenise text without writing the code ourselves.\n",
    "\n",
    "We will also download a specific tokeniser that NLTK uses as default. There are different ways of tokenising text and today we will use NLTK‚Äôs built-in `punkt` tokeniser by calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "crude-smooth",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/balex/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell now (it's fine if you see some pink messages underneath it).\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-employment",
   "metadata": {},
   "source": [
    "As an example, let's tokenise (split into tokens) the nursery rhyme \"Humpty Dumpty\".\n",
    "\n",
    "We will save the tokenised output in a list using the `humpty_tokens` variable so we can inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "monthly-chance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Humpty', 'Dumpty', 'sat', 'on', 'a', 'wall', ',', 'Humpty', 'Dumpty', 'had', 'a', 'great', 'fall', ';', 'All', 'the', 'king', \"'s\", 'horses', 'and', 'all', 'the', 'king', \"'s\", 'men', 'could', \"n't\", 'put', 'Humpty', 'together', 'again', '.']\n"
     ]
    }
   ],
   "source": [
    "humpty_string = \"Humpty Dumpty sat on a wall, Humpty Dumpty had a great fall; All the king's horses and all the king's men couldn't put Humpty together again.\"\n",
    "humpty_tokens = word_tokenize(humpty_string)\n",
    "print(humpty_tokens) # print all tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "funded-correction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Humpty', 'Dumpty', 'sat', 'on', 'a', 'wall', ',', 'Humpty', 'Dumpty', 'had']\n"
     ]
    }
   ],
   "source": [
    "# Let's print just a few of them to have a closer look:\n",
    "print(humpty_tokens[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-collins",
   "metadata": {},
   "source": [
    "### üêõ Minitask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-award",
   "metadata": {},
   "source": [
    "Write some Python code to tokenise the transcript document.  Remember you have already saved that in the ```transcript``` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-picture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-abortion",
   "metadata": {},
   "outputs": [],
   "source": [
    "<details><summary style='color:blue'>CLICK HERE TO SEE THE THE ANSWER. BUT REALLY TRY TO DO IT YOURSELF FIRST!</summary>\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "\n",
    "    ...\n",
    "    \n",
    "    ### END SOLUTION\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-triumph",
   "metadata": {},
   "source": [
    "### Unifying and cleaning up the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-depression",
   "metadata": {},
   "source": [
    "To further analyse the data, we'll first learn how to perform some clean-up tasks. \n",
    "\n",
    "As you can see in the above example, some of the words are uppercase and some are lowercase. But Python is case-sensitive, which means that 'hope' and 'Hope' are considered two completely different strings.\n",
    "\n",
    "For example, when searching for a word or counting the occurrences of a word, we most likely will want to consider both the lowercase and uppercase versions of the word (e.g., `company` and `Company` ). That's why, to simplify the analysis, we often normalise the data by making it all lowercase. This way, both of the above words would simply become `company`, making the text easier to comprehend.\n",
    "\n",
    "Since our list of tokens is a list of strings (words and punctuation) we can apply the `list comprehension loop` we learned about before to transform our list of mixed-case words into a list of lowercase words. \n",
    "\n",
    "As you might remember, a syntax for such loop is `[output_format for item in items ]` where:\n",
    "\n",
    "- `output_format` is some operation we perform on item, like `item.lower()` or `len(item)`\n",
    "- `items` is the list with all the elements we want to transform\n",
    "- `item` is a temporary name we give to each element of `items`, for the purposes of using that name inside of `output_format`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-moscow",
   "metadata": {},
   "source": [
    "Let's modify above example, so that we only work with lowercased tokens of the nursery rhyme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-approach",
   "metadata": {},
   "outputs": [],
   "source": [
    "humpty_string = \"Humpty Dumpty sat on a wall, Humpty Dumpty had a great fall; All the king's horses and all the king's men couldn't put Humpty together again.\"\n",
    "humpty_tokens = word_tokenize(humpty_string)\n",
    "lowercase_tokens = [token.lower() for token in humpty_tokens]\n",
    "print(lowercase_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-staff",
   "metadata": {},
   "source": [
    "### Preparing Your Tokens for Analysis (e.g., making all words lowercase)\n",
    "### A few words about overwhelming your computer with processing: DO NOT PANIC!\n",
    "\n",
    "Note that this dataset is quite large: it contains almost 500 files, 30,000,000 words, and 130,000,000 characters! It's 120MB of data.\n",
    "\n",
    "Still, loading all the files and turning text into tokens takes about 1-2 seconds!\n",
    "\n",
    "However, when we lowercase all of the words in this corpus, we run a piece of code which turns every single character of the 130,000,000 characters into its lowercased equivalent.\n",
    "\n",
    "`lower_corpus_tokens = [word.lower() for word in corpus_tokens]` \n",
    "\n",
    "This is not going to happen instantly. Run the below code cell now, and then keep reading.\n",
    "\n",
    "You'll need to be patient; it might take a minute or more to run. It will look as if your notebook has FROZEN (it might stop responding), but it's just busy at work. You will know that the cell is running, because there will be an `In [*]` on the left top of that busy cell, and (on some browsers) the icon on the browser tab will turn into an hour glass.\n",
    "\n",
    "If you have not done it yet, run the below cell now and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-housing",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_corpus_tokens = [word.lower() for word in corpus_tokens] \n",
    "print(lower_corpus_tokens[400:450]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-seminar",
   "metadata": {},
   "source": [
    "Here are some things you can do to not have to wait all the time for your code to finish:\n",
    "\n",
    "**We try to do the hardest processing only once, and store the result in a variable, which we use later** (instead of re-doing the processing all the time).\n",
    "\n",
    "This is exactly what we do above: after the above line of code had finished running, the variable `lower_corpus_tokens` contains all of your tokens as lowercase characters.\n",
    "\n",
    "Accessing the tokens in this variable will take a very short time now, because all the time-consuming processing (making things lowercase) is already done, and all the tokens are now lowercased.  Moving forward, we'll just want to view them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will execute almost immediately now, because all the processing \n",
    "# (changing millions of characters) is done, and we are just requesting a subset of a (very large) list:\n",
    "\n",
    "print(lower_corpus_tokens[400:450]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-plaintiff",
   "metadata": {},
   "source": [
    "Overall, when processing needs to be done, it needs to be done. So don't be scared of it, and when you see the `In [*]` indicator that a cell is processing, just get a cup of tea :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-ecology",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "# 2. Concordance Lists (tokens in context)\n",
    "\n",
    "#### Questions & Objectives\n",
    "\n",
    "- How can I load a text collection made up of multiple text files and tokenise them?\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "- A concordance list is a list of all contexts in which a particular token appears in a corpus or text.\n",
    "- A concordance list can be created using the `concordance()` method of the `Text` class in NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-novelty",
   "metadata": {},
   "source": [
    "Words make a lot of sense in context. A concordance list is a fantastic way to glimpse into the text and see how a particular token is used.\n",
    "\n",
    "Next, we will display concordances for a particular token, i.e., all contexts a particular token appears in. We can do this using the `Text` class in NLTK‚Äôs `text` package. We can represent our list of lowercased tokens in the document collection loaded previously using the `Text` class. The concordance list of a token can be displayed using the `.concordance()` method on this class as shown below.\n",
    "\n",
    "Note that the process of acquiring concordance data will take about 10 seconds, depending on how busy your machine currently is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-mortality",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.text import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-tampa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_of_the_corpus = Text(lower_corpus_tokens)\n",
    "print(text_of_the_corpus.concordance('woman'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-embassy",
   "metadata": {},
   "source": [
    "In the output for the next bit of code, which creates a concordance list for the word 'he', we can see that there are many more results in the list than displayed on screen ('Displaying 25 of 22830 matches'). The `concordance()` method only prints the first 25 results by default (or less if there are less)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-underground",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_of_the_corpus.concordance('he'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-fitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can specify the number of lines using \n",
    "# an additional lines parameter, e.g.,\n",
    "print(text_of_the_corpus.concordance('he', lines=170))\n",
    "\n",
    "# Notice that when the result of a cell is too long, it will become its own little scrollable window."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-secret",
   "metadata": {},
   "source": [
    "#### üêõMinitask: combining what we learned so far\n",
    "\n",
    "This task will require you to copy-paste and adjust various lines of code from this notebook. We will load and analyse a collection of Barack Obama speeches. \n",
    "\n",
    "- Load a new corpus: a few selected speeches of Barack Obama located in the folder `./data/barack_obama_speeches`. \n",
    "- Turn corpus data into tokens (with `.words()`)\n",
    "- Lowercase all the tokens using a list comprehension loop (`[output for something in list_of somethings]`) and `.lower()`\n",
    "- Create a Text object from the lowercased tokens with  `Text( your_lowercased_tokens )`\n",
    "- Create concordance lists for some words that you find interesting, e.g., 'hope', 'can', 'people'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-abuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-compilation",
   "metadata": {},
   "source": [
    "<details><summary style='color:blue'>CLICK HERE TO SEE THE THE ANSWER. BUT REALLY TRY TO DO IT YOURSELF FIRST!</summary>\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    corpus_root = \"./data/barack_obama_speeches\"\n",
    "    corpus_data = PlaintextCorpusReader(corpus_root, '.*', encoding='latin1') \n",
    "    corpus_tokens = corpus_data.words()\n",
    "    corpus_text_object = Text(lower_india_tokens)\n",
    "    print(corpus_text_object[:50])\n",
    "    print(corpus_text_object.concordance('people'))\n",
    "    ### END SOLUTION\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-validation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
